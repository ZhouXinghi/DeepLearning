{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "def corr2d(X, K):\n",
    "    \"\"\"correlation2d\"\"\"\n",
    "    h, w = K.shape\n",
    "    Y = torch.zeros(size=(X.shape[0] - h + 1, X.shape[1] - w + 1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            Y[i, j] = (X[i:i+h, j:j+w] * K).sum()\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[19., 25.],\n",
       "        [37., 43.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([[0, 1, 2], [3, 4, 5], [6, 7, 8]], dtype=torch.float32)\n",
    "K = torch.tensor([[0, 1], [2, 3]], dtype=torch.float32)\n",
    "corr2d(X, K)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 手写Conv2d层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2D(nn.Module):\n",
    "    def __init__(self, kernel_size):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.rand(kernel_size))\n",
    "        self.bias = nn.Parameter(torch.zeros(1))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return corr2d(X, self.weight) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "         [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "         [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "         [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "         [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "         [1., 1., 0., 0., 0., 0., 1., 1.]]),\n",
       " tensor([[ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "         [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "         [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "         [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "         [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "         [ 0.,  1.,  0.,  0.,  0., -1.,  0.]]),\n",
       " tensor(0.5000))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.ones((6, 8))\n",
    "X[:, 2:6] = 0\n",
    "K = torch.tensor([[1, -1]], dtype=torch.float32)\n",
    "Y = corr2d(X, K)\n",
    "X, Y, X.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 1 loss: 0.4197260\n",
      "batch 2 loss: 0.3578563\n",
      "batch 3 loss: 0.3096138\n",
      "batch 4 loss: 0.2715398\n",
      "batch 5 loss: 0.2410849\n",
      "batch 6 loss: 0.2163675\n",
      "batch 7 loss: 0.1959972\n",
      "batch 8 loss: 0.1789454\n",
      "batch 9 loss: 0.1644490\n",
      "batch 10 loss: 0.1519411\n",
      "batch 11 loss: 0.1409986\n",
      "batch 12 loss: 0.1313050\n",
      "batch 13 loss: 0.1226221\n",
      "batch 14 loss: 0.1147697\n",
      "batch 15 loss: 0.1076105\n",
      "batch 16 loss: 0.1010388\n",
      "batch 17 loss: 0.0949727\n",
      "batch 18 loss: 0.0893478\n",
      "batch 19 loss: 0.0841127\n",
      "batch 20 loss: 0.0792262\n",
      "batch 21 loss: 0.0746544\n",
      "batch 22 loss: 0.0703692\n",
      "batch 23 loss: 0.0663465\n",
      "batch 24 loss: 0.0625661\n",
      "batch 25 loss: 0.0590102\n",
      "batch 26 loss: 0.0556630\n",
      "batch 27 loss: 0.0525105\n",
      "batch 28 loss: 0.0495401\n",
      "batch 29 loss: 0.0467404\n",
      "batch 30 loss: 0.0441009\n",
      "batch 31 loss: 0.0416118\n",
      "batch 32 loss: 0.0392643\n",
      "batch 33 loss: 0.0370500\n",
      "batch 34 loss: 0.0349611\n",
      "batch 35 loss: 0.0329904\n",
      "batch 36 loss: 0.0311311\n",
      "batch 37 loss: 0.0293768\n",
      "batch 38 loss: 0.0277215\n",
      "batch 39 loss: 0.0261596\n",
      "batch 40 loss: 0.0246859\n",
      "batch 41 loss: 0.0232952\n",
      "batch 42 loss: 0.0219829\n",
      "batch 43 loss: 0.0207445\n",
      "batch 44 loss: 0.0195759\n",
      "batch 45 loss: 0.0184732\n",
      "batch 46 loss: 0.0174327\n",
      "batch 47 loss: 0.0164507\n",
      "batch 48 loss: 0.0155241\n",
      "batch 49 loss: 0.0146496\n",
      "batch 50 loss: 0.0138244\n",
      "batch 51 loss: 0.0130458\n",
      "batch 52 loss: 0.0123109\n",
      "batch 53 loss: 0.0116175\n",
      "batch 54 loss: 0.0109631\n",
      "batch 55 loss: 0.0103456\n",
      "batch 56 loss: 0.0097629\n",
      "batch 57 loss: 0.0092129\n",
      "batch 58 loss: 0.0086940\n",
      "batch 59 loss: 0.0082043\n",
      "batch 60 loss: 0.0077422\n",
      "batch 61 loss: 0.0073061\n",
      "batch 62 loss: 0.0068946\n",
      "batch 63 loss: 0.0065062\n",
      "batch 64 loss: 0.0061398\n",
      "batch 65 loss: 0.0057939\n",
      "batch 66 loss: 0.0054676\n",
      "batch 67 loss: 0.0051596\n",
      "batch 68 loss: 0.0048690\n",
      "batch 69 loss: 0.0045947\n",
      "batch 70 loss: 0.0043359\n",
      "batch 71 loss: 0.0040917\n",
      "batch 72 loss: 0.0038612\n",
      "batch 73 loss: 0.0036437\n",
      "batch 74 loss: 0.0034385\n",
      "batch 75 loss: 0.0032448\n",
      "batch 76 loss: 0.0030620\n",
      "batch 77 loss: 0.0028896\n",
      "batch 78 loss: 0.0027268\n",
      "batch 79 loss: 0.0025732\n",
      "batch 80 loss: 0.0024283\n",
      "batch 81 loss: 0.0022915\n",
      "batch 82 loss: 0.0021624\n",
      "batch 83 loss: 0.0020406\n",
      "batch 84 loss: 0.0019257\n",
      "batch 85 loss: 0.0018172\n",
      "batch 86 loss: 0.0017149\n",
      "batch 87 loss: 0.0016183\n",
      "batch 88 loss: 0.0015271\n",
      "batch 89 loss: 0.0014411\n",
      "batch 90 loss: 0.0013599\n",
      "batch 91 loss: 0.0012833\n",
      "batch 92 loss: 0.0012110\n",
      "batch 93 loss: 0.0011428\n",
      "batch 94 loss: 0.0010785\n",
      "batch 95 loss: 0.0010177\n",
      "batch 96 loss: 0.0009604\n",
      "batch 97 loss: 0.0009063\n",
      "batch 98 loss: 0.0008552\n",
      "batch 99 loss: 0.0008071\n",
      "batch 100 loss: 0.0007616\n",
      "batch 101 loss: 0.0007187\n",
      "batch 102 loss: 0.0006782\n",
      "batch 103 loss: 0.0006400\n",
      "batch 104 loss: 0.0006040\n",
      "batch 105 loss: 0.0005700\n",
      "batch 106 loss: 0.0005379\n",
      "batch 107 loss: 0.0005076\n",
      "batch 108 loss: 0.0004790\n",
      "batch 109 loss: 0.0004520\n",
      "batch 110 loss: 0.0004265\n",
      "batch 111 loss: 0.0004025\n",
      "batch 112 loss: 0.0003798\n",
      "batch 113 loss: 0.0003584\n",
      "batch 114 loss: 0.0003382\n",
      "batch 115 loss: 0.0003192\n",
      "batch 116 loss: 0.0003012\n",
      "batch 117 loss: 0.0002842\n",
      "batch 118 loss: 0.0002682\n",
      "batch 119 loss: 0.0002531\n",
      "batch 120 loss: 0.0002389\n",
      "batch 121 loss: 0.0002254\n",
      "batch 122 loss: 0.0002127\n",
      "batch 123 loss: 0.0002007\n",
      "batch 124 loss: 0.0001894\n",
      "batch 125 loss: 0.0001788\n",
      "batch 126 loss: 0.0001687\n",
      "batch 127 loss: 0.0001592\n",
      "batch 128 loss: 0.0001502\n",
      "batch 129 loss: 0.0001418\n",
      "batch 130 loss: 0.0001338\n",
      "batch 131 loss: 0.0001262\n",
      "batch 132 loss: 0.0001191\n",
      "batch 133 loss: 0.0001124\n",
      "batch 134 loss: 0.0001061\n",
      "batch 135 loss: 0.0001001\n",
      "batch 136 loss: 0.0000945\n",
      "batch 137 loss: 0.0000892\n",
      "batch 138 loss: 0.0000841\n",
      "batch 139 loss: 0.0000794\n",
      "batch 140 loss: 0.0000749\n",
      "batch 141 loss: 0.0000707\n",
      "batch 142 loss: 0.0000667\n",
      "batch 143 loss: 0.0000630\n",
      "batch 144 loss: 0.0000594\n",
      "batch 145 loss: 0.0000561\n",
      "batch 146 loss: 0.0000529\n",
      "batch 147 loss: 0.0000499\n",
      "batch 148 loss: 0.0000471\n",
      "batch 149 loss: 0.0000445\n",
      "batch 150 loss: 0.0000420\n",
      "batch 151 loss: 0.0000396\n",
      "batch 152 loss: 0.0000374\n",
      "batch 153 loss: 0.0000353\n",
      "batch 154 loss: 0.0000333\n",
      "batch 155 loss: 0.0000314\n",
      "batch 156 loss: 0.0000296\n",
      "batch 157 loss: 0.0000280\n",
      "batch 158 loss: 0.0000264\n",
      "batch 159 loss: 0.0000249\n",
      "batch 160 loss: 0.0000235\n",
      "batch 161 loss: 0.0000222\n",
      "batch 162 loss: 0.0000209\n",
      "batch 163 loss: 0.0000197\n",
      "batch 164 loss: 0.0000186\n",
      "batch 165 loss: 0.0000176\n",
      "batch 166 loss: 0.0000166\n",
      "batch 167 loss: 0.0000157\n",
      "batch 168 loss: 0.0000148\n",
      "batch 169 loss: 0.0000139\n",
      "batch 170 loss: 0.0000132\n",
      "batch 171 loss: 0.0000124\n",
      "batch 172 loss: 0.0000117\n",
      "batch 173 loss: 0.0000111\n",
      "batch 174 loss: 0.0000104\n",
      "batch 175 loss: 0.0000098\n",
      "batch 176 loss: 0.0000093\n",
      "batch 177 loss: 0.0000088\n",
      "batch 178 loss: 0.0000083\n",
      "batch 179 loss: 0.0000078\n",
      "batch 180 loss: 0.0000074\n",
      "batch 181 loss: 0.0000070\n",
      "batch 182 loss: 0.0000066\n",
      "batch 183 loss: 0.0000062\n",
      "batch 184 loss: 0.0000058\n",
      "batch 185 loss: 0.0000055\n",
      "batch 186 loss: 0.0000052\n",
      "batch 187 loss: 0.0000049\n",
      "batch 188 loss: 0.0000046\n",
      "batch 189 loss: 0.0000044\n",
      "batch 190 loss: 0.0000041\n",
      "batch 191 loss: 0.0000039\n",
      "batch 192 loss: 0.0000037\n",
      "batch 193 loss: 0.0000035\n",
      "batch 194 loss: 0.0000033\n",
      "batch 195 loss: 0.0000031\n",
      "batch 196 loss: 0.0000029\n",
      "batch 197 loss: 0.0000028\n",
      "batch 198 loss: 0.0000026\n",
      "batch 199 loss: 0.0000024\n",
      "batch 200 loss: 0.0000023\n",
      "batch 201 loss: 0.0000022\n",
      "batch 202 loss: 0.0000021\n",
      "batch 203 loss: 0.0000019\n",
      "batch 204 loss: 0.0000018\n",
      "batch 205 loss: 0.0000017\n",
      "batch 206 loss: 0.0000016\n",
      "batch 207 loss: 0.0000015\n",
      "batch 208 loss: 0.0000015\n",
      "batch 209 loss: 0.0000014\n",
      "batch 210 loss: 0.0000013\n",
      "batch 211 loss: 0.0000012\n",
      "batch 212 loss: 0.0000012\n",
      "batch 213 loss: 0.0000011\n",
      "batch 214 loss: 0.0000010\n",
      "batch 215 loss: 0.0000010\n",
      "batch 216 loss: 0.0000009\n",
      "batch 217 loss: 0.0000009\n",
      "batch 218 loss: 0.0000008\n",
      "batch 219 loss: 0.0000008\n",
      "batch 220 loss: 0.0000007\n",
      "batch 221 loss: 0.0000007\n",
      "batch 222 loss: 0.0000006\n",
      "batch 223 loss: 0.0000006\n",
      "batch 224 loss: 0.0000006\n",
      "batch 225 loss: 0.0000005\n",
      "batch 226 loss: 0.0000005\n",
      "batch 227 loss: 0.0000005\n",
      "batch 228 loss: 0.0000005\n",
      "batch 229 loss: 0.0000004\n",
      "batch 230 loss: 0.0000004\n",
      "batch 231 loss: 0.0000004\n",
      "batch 232 loss: 0.0000004\n",
      "batch 233 loss: 0.0000003\n",
      "batch 234 loss: 0.0000003\n",
      "batch 235 loss: 0.0000003\n",
      "batch 236 loss: 0.0000003\n",
      "batch 237 loss: 0.0000003\n",
      "batch 238 loss: 0.0000003\n",
      "batch 239 loss: 0.0000002\n",
      "batch 240 loss: 0.0000002\n",
      "batch 241 loss: 0.0000002\n",
      "batch 242 loss: 0.0000002\n",
      "batch 243 loss: 0.0000002\n",
      "batch 244 loss: 0.0000002\n",
      "batch 245 loss: 0.0000002\n",
      "batch 246 loss: 0.0000002\n",
      "batch 247 loss: 0.0000002\n",
      "batch 248 loss: 0.0000001\n",
      "batch 249 loss: 0.0000001\n",
      "batch 250 loss: 0.0000001\n",
      "batch 251 loss: 0.0000001\n",
      "batch 252 loss: 0.0000001\n",
      "batch 253 loss: 0.0000001\n",
      "batch 254 loss: 0.0000001\n",
      "batch 255 loss: 0.0000001\n",
      "batch 256 loss: 0.0000001\n",
      "batch 257 loss: 0.0000001\n",
      "batch 258 loss: 0.0000001\n",
      "batch 259 loss: 0.0000001\n",
      "batch 260 loss: 0.0000001\n",
      "batch 261 loss: 0.0000001\n",
      "batch 262 loss: 0.0000001\n",
      "batch 263 loss: 0.0000001\n",
      "batch 264 loss: 0.0000001\n",
      "batch 265 loss: 0.0000001\n",
      "batch 266 loss: 0.0000001\n",
      "batch 267 loss: 0.0000000\n",
      "batch 268 loss: 0.0000000\n",
      "batch 269 loss: 0.0000000\n",
      "batch 270 loss: 0.0000000\n",
      "batch 271 loss: 0.0000000\n",
      "batch 272 loss: 0.0000000\n",
      "batch 273 loss: 0.0000000\n",
      "batch 274 loss: 0.0000000\n",
      "batch 275 loss: 0.0000000\n",
      "batch 276 loss: 0.0000000\n",
      "batch 277 loss: 0.0000000\n",
      "batch 278 loss: 0.0000000\n",
      "batch 279 loss: 0.0000000\n",
      "batch 280 loss: 0.0000000\n",
      "batch 281 loss: 0.0000000\n",
      "batch 282 loss: 0.0000000\n",
      "batch 283 loss: 0.0000000\n",
      "batch 284 loss: 0.0000000\n",
      "batch 285 loss: 0.0000000\n",
      "batch 286 loss: 0.0000000\n",
      "batch 287 loss: 0.0000000\n",
      "batch 288 loss: 0.0000000\n",
      "batch 289 loss: 0.0000000\n",
      "batch 290 loss: 0.0000000\n",
      "batch 291 loss: 0.0000000\n",
      "batch 292 loss: 0.0000000\n",
      "batch 293 loss: 0.0000000\n",
      "batch 294 loss: 0.0000000\n",
      "batch 295 loss: 0.0000000\n",
      "batch 296 loss: 0.0000000\n",
      "batch 297 loss: 0.0000000\n",
      "batch 298 loss: 0.0000000\n",
      "batch 299 loss: 0.0000000\n",
      "batch 300 loss: 0.0000000\n",
      "batch 301 loss: 0.0000000\n",
      "batch 302 loss: 0.0000000\n",
      "batch 303 loss: 0.0000000\n",
      "batch 304 loss: 0.0000000\n",
      "batch 305 loss: 0.0000000\n",
      "batch 306 loss: 0.0000000\n",
      "batch 307 loss: 0.0000000\n",
      "batch 308 loss: 0.0000000\n",
      "batch 309 loss: 0.0000000\n",
      "batch 310 loss: 0.0000000\n",
      "batch 311 loss: 0.0000000\n",
      "batch 312 loss: 0.0000000\n",
      "batch 313 loss: 0.0000000\n",
      "batch 314 loss: 0.0000000\n",
      "batch 315 loss: 0.0000000\n",
      "batch 316 loss: 0.0000000\n",
      "batch 317 loss: 0.0000000\n",
      "batch 318 loss: 0.0000000\n",
      "batch 319 loss: 0.0000000\n",
      "batch 320 loss: 0.0000000\n",
      "batch 321 loss: 0.0000000\n",
      "batch 322 loss: 0.0000000\n",
      "batch 323 loss: 0.0000000\n",
      "batch 324 loss: 0.0000000\n",
      "batch 325 loss: 0.0000000\n",
      "batch 326 loss: 0.0000000\n",
      "batch 327 loss: 0.0000000\n",
      "batch 328 loss: 0.0000000\n",
      "batch 329 loss: 0.0000000\n",
      "batch 330 loss: 0.0000000\n",
      "batch 331 loss: 0.0000000\n",
      "batch 332 loss: 0.0000000\n",
      "batch 333 loss: 0.0000000\n",
      "batch 334 loss: 0.0000000\n",
      "batch 335 loss: 0.0000000\n",
      "batch 336 loss: 0.0000000\n",
      "batch 337 loss: 0.0000000\n",
      "batch 338 loss: 0.0000000\n",
      "batch 339 loss: 0.0000000\n",
      "batch 340 loss: 0.0000000\n",
      "batch 341 loss: 0.0000000\n",
      "batch 342 loss: 0.0000000\n",
      "batch 343 loss: 0.0000000\n",
      "batch 344 loss: 0.0000000\n",
      "batch 345 loss: 0.0000000\n",
      "batch 346 loss: 0.0000000\n",
      "batch 347 loss: 0.0000000\n",
      "batch 348 loss: 0.0000000\n",
      "batch 349 loss: 0.0000000\n",
      "batch 350 loss: 0.0000000\n",
      "batch 351 loss: 0.0000000\n",
      "batch 352 loss: 0.0000000\n",
      "batch 353 loss: 0.0000000\n",
      "batch 354 loss: 0.0000000\n",
      "batch 355 loss: 0.0000000\n",
      "batch 356 loss: 0.0000000\n",
      "batch 357 loss: 0.0000000\n",
      "batch 358 loss: 0.0000000\n",
      "batch 359 loss: 0.0000000\n",
      "batch 360 loss: 0.0000000\n",
      "batch 361 loss: 0.0000000\n",
      "batch 362 loss: 0.0000000\n",
      "batch 363 loss: 0.0000000\n",
      "batch 364 loss: 0.0000000\n",
      "batch 365 loss: 0.0000000\n",
      "batch 366 loss: 0.0000000\n",
      "batch 367 loss: 0.0000000\n",
      "batch 368 loss: 0.0000000\n",
      "batch 369 loss: 0.0000000\n",
      "batch 370 loss: 0.0000000\n",
      "batch 371 loss: 0.0000000\n",
      "batch 372 loss: 0.0000000\n",
      "batch 373 loss: 0.0000000\n",
      "batch 374 loss: 0.0000000\n",
      "batch 375 loss: 0.0000000\n",
      "batch 376 loss: 0.0000000\n",
      "batch 377 loss: 0.0000000\n",
      "batch 378 loss: 0.0000000\n",
      "batch 379 loss: 0.0000000\n",
      "batch 380 loss: 0.0000000\n",
      "batch 381 loss: 0.0000000\n",
      "batch 382 loss: 0.0000000\n",
      "batch 383 loss: 0.0000000\n",
      "batch 384 loss: 0.0000000\n",
      "batch 385 loss: 0.0000000\n",
      "batch 386 loss: 0.0000000\n",
      "batch 387 loss: 0.0000000\n",
      "batch 388 loss: 0.0000000\n",
      "batch 389 loss: 0.0000000\n",
      "batch 390 loss: 0.0000000\n",
      "batch 391 loss: 0.0000000\n",
      "batch 392 loss: 0.0000000\n",
      "batch 393 loss: 0.0000000\n",
      "batch 394 loss: 0.0000000\n",
      "batch 395 loss: 0.0000000\n",
      "batch 396 loss: 0.0000000\n",
      "batch 397 loss: 0.0000000\n",
      "batch 398 loss: 0.0000000\n",
      "batch 399 loss: 0.0000000\n",
      "batch 400 loss: 0.0000000\n",
      "tensor([[[[ 1.0000, -1.0000]]]])\n",
      "tensor([[[[-2.4693e-06,  2.4693e-06]]]])\n"
     ]
    }
   ],
   "source": [
    "conv2d = nn.Conv2d(1, 1, kernel_size=(1, 2), bias=False)\n",
    "X = X.reshape((1, 1, 6, 8))\n",
    "Y = Y.reshape((1, 1, 6, 7))\n",
    "\n",
    "for i in range(400):\n",
    "    Y_hat = conv2d(X)\n",
    "    l = ((Y_hat - Y) ** 2).mean()\n",
    "    conv2d.zero_grad()\n",
    "    l.backward()\n",
    "    conv2d.weight.data -= 0.1 * conv2d.weight.grad\n",
    "    print(f'batch {i+1} loss: {l:.7f}')\n",
    "\n",
    "print(conv2d.weight.data)\n",
    "print(conv2d.weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3fa4b62426a0cdf1c02e81afc6355f8e9b3dd70393a728624f053bfcdc4160eb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
